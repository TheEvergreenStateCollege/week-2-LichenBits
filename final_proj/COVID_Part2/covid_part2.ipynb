{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7742c77f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"covid_part2.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b82b09",
   "metadata": {},
   "source": [
    "# Final Project: COVID-19 Dataset\n",
    "## Exploring COVID-19 Data through Modeling\n",
    "## Due Date: Thursday, December 13th, 11:59 PM\n",
    "## Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with other groups about\n",
    "the project, we ask that you **write your solutions individually**. If you do\n",
    "discuss the assignments with others outside of your group please **include their names** at the top\n",
    "of your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb908b16",
   "metadata": {},
   "source": [
    "\n",
    "## This Assignment\n",
    "\n",
    "In this final project, we will investigate COVID-19 data over the past year. This data contains information about COVID-19 case counts, mortalities, vaccination rates, and various other metadata that can assist in modeling various aspects of COVID-19.\n",
    "\n",
    "Through this final project, you will demonstrate your experience with:\n",
    "* Data cleaning and EDA using Pandas\n",
    "* Unsupervised and supervised learning techniques\n",
    "* Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2811cb",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Model and analyze the temporal evolution of COVID-19 mortalities or cases using one unsupervised and one supervised technique of your choice. Interpret your models' results through visualizations, and draw insightful conclusions about the modeling of COVID-19 data.\n",
    "\n",
    "Recall that we studied linear and logistic regression, decision trees, random forests as part of supervised learning (with labels) and clustering, PCA as part of unsupervised learning (without labels). You are free to use any methods that you find suitable to answer the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c01af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "import numpy as np\n",
    "from geopy import *\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import pearsonr\n",
    "import re\n",
    "\n",
    "cases = pd.read_csv('data/time_series_covid19_confirmed_US.csv') # https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv\n",
    "vaccinations = pd.read_csv('data/people_vaccinated_us_timeline.csv') # https://raw.githubusercontent.com/govex/COVID-19/master/data_tables/vaccine_data/us_data/time_series/people_vaccinated_us_timeline.csv\n",
    "counties = pd.read_csv('data/co-est2020.csv', encoding='latin-1') # https://www2.census.gov/programs-surveys/popest/datasets/2010-2020/counties/totals/co-est2020.csv\n",
    "mask_use = pd.read_csv('data/mask-use-by-county.csv') # https://github.com/nytimes/covid-19-data/blob/master/mask-use/mask-use-by-county.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4b18d3",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "## Data Cleaning (Again!)\n",
    "\n",
    "For this section, please copy over the appropriate answers from your previous notebook submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e6efb9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Part 1: Question 1a\n",
    "\n",
    "Impute the null values in *all* the datasets with zero values or empty strings where appropriate.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a\n",
    "points: 0\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd43134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42e23fb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fdc73c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Part 1: Question 1d\n",
    "\n",
    "Generate a valid FIPS code for the `counties` table.\n",
    "\n",
    "*Hint*: Refer to [this](https://transition.fcc.gov/oet/info/maps/census/fips/fips.txt) guide on FIPS codes.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1d\n",
    "points: 0\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8869affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f936214a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5851d0b6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Part 1: Question 1e\n",
    "\n",
    "Merge the `counties`, `cases`, and `mask_use` tables on an appropriate primary key to generate county-wise data.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1e\n",
    "points: 0\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3730e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_data = ...\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a0793",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb5b84",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "## Question 5: Guided Supervised Modeling\n",
    "\n",
    "This section will guide you through creating a supervised learning framework that will predict the number of COVID-19 cases per capita given various COVID-19 safety protocols that have been implemented. Then, we will investigate the bias, variance, and observational noise of this framework.\n",
    "\n",
    "Note that any answer responses without the appropriate work (i.e. code or math) will be subject to additional review and will not receive any credit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94355a0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 5a\n",
    "\n",
    "We will use county-wise mask usage data to predict the number of COVID-19 cases on September 12th, 2021. Create a visualization that shows the pairwise correlation between each combination of column in the mask usage data and the number of COVID-19 cases.\n",
    "\n",
    "*Hint*: You should be plotting 36 correlations.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5a\n",
    "points: 3\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98cbca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8860eb5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Question 5b\n",
    "\n",
    "Train a linear regression model to predict the number of COVID-19 cases using county-wise mask usage data for September 12, 2021. Evaluate your model's RMSE on a held-out validation set with 33% of the county-wise data. When possible, make sure to set `random_state = 42` when splitting your data into training and test sets.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5b\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c917813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_q5b = ...\n",
    "y_q5b = ...\n",
    "\n",
    "# Make sure to set random_state = 42 and test_size = 0.33!\n",
    "X_q5b_train, X_q5b_test, y_q5b_train, y_q5b_test = ..., ..., ..., ...\n",
    "\n",
    "...\n",
    "\n",
    "train_rmse_cases, test_rmse_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3bcbd5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07da3a5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 5c\n",
    "\n",
    "Explain potential reasons the test set RMSE is much higher as compared to the training set RMSE.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5c\n",
    "points: 3\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd63a7d",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbbb97c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Question 5d\n",
    "\n",
    "Instead of predicting the number of COVID-19 cases, redo part (b) by predicting the number of cases per capita. Report the model's RMSE on the training and validation set.\n",
    "\n",
    "Comment on the relationship between the training and test RMSE by predicting the number of cases per capita instead of the total number of cases.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5d\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85dfd858",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_q5d = ...\n",
    "y_q5d = ...\n",
    "X_q5d_train, X_q5d_test, y_q5d_train, y_q5d_test = ..., ..., ..., ...\n",
    "\n",
    "...\n",
    "\n",
    "train_rmse_cpc, test_rmse_cpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbcb703",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2215f3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 5e\n",
    "\n",
    "Visualize the model outputs from part (d) by plotting the predictions $\\hat{y}$ versus the observations $y$. Comment on what the plot indicates about our linear model as a comment in the code cell.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5e\n",
    "points: 3\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e59258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb5469",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Question 5f\n",
    "\n",
    "We will investigate the bias and variance of this improved model on the test set using the bias-variance decomposition to formalize the behaviour of our model. To generate an empirical estimate of the errors and the parameters in the bias-variance decomposition, train 1000 bootstrapped models on the training dataset from part (d).\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5f\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7b7619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5211469f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715760e0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 5g\n",
    "\n",
    "To investigate the variance in our test predictions, we sample a particular test point $(x_i, y_i)$ such that $i = 100$. In other words, we will use the 100th point in the test set from part (d), `(X_q5d_test.iloc[100], y_q5d_test.iloc[100])` as the testing point.\n",
    "\n",
    "Generate predictions and square errors for this test point for all 1000 models, and calculate the *proportion* of the *expected* square error that is captured by the model prediction variance. In other words, we wish to estimate the following quantity:\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{Var}(f_\\theta(x_i))}{\\mathrm{E}_\\theta[(y_i - f_\\theta(x_i))^2]}\n",
    "$$\n",
    "\n",
    "*Hint*: Refer to the bias-variance decomposition from lecture.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5g\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "896ac261",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_var = ...\n",
    "\n",
    "...\n",
    "prop_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7bcc0c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b590da8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 5h\n",
    "\n",
    "Using the bias-variance decomposition, comment on how much the variance of the model contributes to the error on the sample point above. We will extend this scenario to analyze the noise term in the bias-variance decomposition, specifically with regards to this COVID-19 dataset. Consider the following:\n",
    "\n",
    "i) Assuming no observational noise (i.e. $\\epsilon = 0$), what is the *magnitude* of the empirical model bias on this sample point?\n",
    "\n",
    "ii) Clearly, there is a non-trivial amount of observational noise with COVID-19 case data simply due to how testing works and occurs. Please take a look at [this article](https://fivethirtyeight.com/features/coronavirus-case-counts-are-meaningless/) for more information. Given this infomation, explain the issues with the assumptions and result in 5h(i).\n",
    "\n",
    "iii) Recall that we typically assume $y = g(x) + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(\\mu, \\sigma)$. In the theoretical setting for bias-variance, we have assumed $\\mu = 0, \\sigma > 0$. In this practical setting, analyze and determine how $\\epsilon$ could be modeled (as a normal distribution, but you may also consider how it could be modeled as another distribution). Are there any immediate issues with the assumptions we have made in the theoretical setting where $\\mu = 0, \\sigma > 0$? What conditions on $\\mu, \\sigma$ could be more appropriate and why?\n",
    "\n",
    "iv) Does the standard bias-variance decomposition presented in lecture hold given $\\epsilon$ being normally distributed according to your answer in part (iii)? If so, please explain why. If not, explain why it does not hold and if possible, how to modify the bias-variance decomposition to make it hold (i.e. perhaps there is an additional noise term $E[\\epsilon^3]$). \n",
    "\n",
    "*Hint*: Try to express $y = g(x) + \\epsilon$ by adding and subtracting a certain quantity.\n",
    "\n",
    "v) Intuitively, is it possible to minimize the true model bias to 0 given your $\\epsilon$ formulation in part (iii)? Why or why not? Justify your answer using part (iv) if possible.\n",
    "\n",
    "vi) Consider the infinite sample case where an oracle grants you as many samples as requested, and answer the same question in part (v). Is it possible to minimize the true model bias to 0 given your $\\epsilon$ formulation in part (iii)? Conclude with an analysis of what our modeling task can approximate using $X\\theta \\approx y$ in the finite and infinite sample case.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5h\n",
    "points: 24\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f478bb92",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653f5fdb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Question 5i\n",
    "\n",
    "Using the bias-variance decomposition for each test point, calculate the average variance and average test mean-square error across the entire test set from part (d). In other words, estimate the following quantities:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_i \\mathrm{Var}(f_\\theta(x_i))\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_i \\mathrm{E}_\\theta[(y_i - f_\\theta(x_i))^2]\n",
    "$$\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5i\n",
    "points: 5\n",
    "manual: False\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a207935",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_var, avg_mse = ..., ...\n",
    "\n",
    "...\n",
    "avg_var, avg_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243a140",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6f530",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 5j\n",
    "\n",
    "Propose a solution to reducing the mean square error using the insights gained from the bias-variance decomposition above. What are the values of the quantities that have we estimated and what can be concluded about the remaining quantities? Please show all work that informs your analysis.\n",
    "\n",
    "Assume that the standard bias-variance decomposition used in lecture can be applied here.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5j\n",
    "points: 5\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bbb2f5",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f0f9b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "## Question 6: Open Supervised Modeling\n",
    "\n",
    "We wish to extend our modeling framework from Question 5 to make it more accurate; in other words, we wish to predict $f(x)$, a supervised learning output, based on past and current quantities. \n",
    "\n",
    "This section will serve as a rough guide on creating an autoregressive modeling framework for predicting a COVID-19 quantity of your choice (i.e. deaths, cases, vaccinations).\n",
    "\n",
    "Note that if you do not wish to pursue time-based modeling of COVID-19, you may skip parts (d), (e), and (f). That being said, you are strongly encouraged to incorporate time-based modeling into your open-ended modeling design since it constitutes a large component of the provided datasets.\n",
    "\n",
    "We will ***not*** grade these below questions individually (i.e. there are no points explicitly assigned to questions 6(a) to 6(f)); they are simply guiding questions and will be graded as part of the final project report. You should make sure to answer all of the questions (that are applicable to your open-ended modeling) in some form in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e3e64",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 6a\n",
    "\n",
    "Train a baseline model where $f$ is the model described in Question 0a and $x$ is a quantity of *your* choice. Note that you may used *any* supervised learning approach we have studied; you are not limited to a linear model.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6a\n",
    "points: 0\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85819006",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3b8a40",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 6b\n",
    "\n",
    "Improve your model from part (a). Specify the supervised model you choose and write $f(x)$ as a function of the chosen features and parameters in your model. Justify why you chose these features and how you expect they will correlate with the output you wish to predict.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6b\n",
    "points: 0\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab91b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28de6f8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 6c\n",
    "\n",
    "If applicable, write an equation or rule for the prediction function $f(x)$; if this is infeasible, make sure to visualize your model parameters in some way. Interpret your improved model's optimal parameters (*hint*: refer to 1aiii), and compare these parameters to those of the baseline model. Comment on whether the parameters follow physical intuition given the nature of the prediction task.\n",
    "\n",
    "For example, if you chose to use a decision tree, you may interpret the generated rules.\n",
    " \n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6c\n",
    "points: 0\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0defa5",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b89e46",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 6d\n",
    "\n",
    "Discuss your improved model's performance on both short-term and long-term time scales using a metric of your choice (*hint:* we're using an autoregressive model). In other words, given $x_t$, we wish to predict $\\hat{x}_{t+k}$, and plot the error of these predictions for two $k$ values of your choice. You may use any reasonable interpretation of short-term and long-term predictions; an initial suggestion is to use 2-day predictions and 2-week predictions.\n",
    "\n",
    "Compare the performance of this model on both timescales with the baseline model.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6d\n",
    "points: 0\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e7549",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ba1ff2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 6e\n",
    "\n",
    "Plot and describe the error for both the baseline and improved models as a function of time. In other words, given $x_t$, we wish to predict $\\hat{x}_{t+k}$, and plot the error of these predictions for all $k$.\n",
    "\n",
    "Consider how and why the performance of the model degrades as a function of time using the rate of growth in the error.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6e\n",
    "points: 0\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526caa49",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 6f\n",
    "\n",
    "Consider a modification to the model $f(x) = x_{t+1}$ where instead $f(x) = [x_{t+1}, x_{t+2}, ..., x_{t+m}]$ for some $m > 1$. In other words, using the features $x$ that contain past and present quantities, our model *explicitly* predicts values for $m$ days in the future rather than simply the next day (i.e. $m = 1$). \n",
    "\n",
    "Train the baseline and improved model using $m = 5$ and $m = 10$. Evaluate and visualize the predictive accuracy of both models.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6f\n",
    "points: 0\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8bc5f0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd82aee6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecfb50d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febbeabf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5501bd",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
